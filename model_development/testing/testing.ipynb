{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrirO2x3q5Pc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_JT3kk-rCrq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"lordgrim18/llama2-elevate-story-3\"\n",
        "prompt = \"generate a story of the horror genre\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWKe68qIrCbW"
      },
      "outputs": [],
      "source": [
        "genre = 'Fantasy'\n",
        "prompt = f\"Write a short story on the genre {genre} with max word count of eighty words\"\n",
        "prompt_template=f'''SYSTEM: You are a helpful and creative story generator assistant that generates a story with content having word count in the range of seventy to eighty words. Only generate a single story.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdYOyiwdrCYl"
      },
      "outputs": [],
      "source": [
        "sequences = pipeline(\n",
        "    prompt_template,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=250,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
